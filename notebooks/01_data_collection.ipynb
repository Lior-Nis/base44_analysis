{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Base44 Phenomenon Analysis - Data Collection\n",
    "\n",
    "This notebook demonstrates the data collection process for Base44 applications using web scraping techniques.\n",
    "\n",
    "## Research Question\n",
    "\"Base44 has become a phenomenon; we need to analyze the different types of projects using the tool and their level\"\n",
    "\n",
    "## Objectives\n",
    "1. Scrape Base44 applications from multiple sources\n",
    "2. Collect comprehensive app metadata\n",
    "3. Store data in structured format for analysis\n",
    "4. Validate data quality and completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Import custom modules\n",
    "from base44_scraper import Base44Scraper\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Analysis started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-collection-strategy",
   "metadata": {},
   "source": [
    "## Data Collection Strategy\n",
    "\n",
    "We will collect Base44 application data from multiple sources:\n",
    "\n",
    "1. **Base44.com website** - Official showcase/gallery\n",
    "2. **Product Hunt** - Base44 app launches\n",
    "3. **Social Media** - Twitter/X mentions\n",
    "4. **Web Search** - Google searches for Base44 apps\n",
    "5. **Community Sources** - Forums and Discord\n",
    "\n",
    "### Data Points Collected\n",
    "- App name and URL\n",
    "- Description/purpose\n",
    "- Category (MVP, internal tool, portal, SaaS replacement)\n",
    "- Creation date (if available)\n",
    "- Creator information\n",
    "- Industry/domain\n",
    "- Features mentioned\n",
    "- User testimonials/reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initialize-scraper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Base44 scraper with rate limiting\n",
    "scraper = Base44Scraper(rate_limit=2.0)  # 2 second delay between requests\n",
    "\n",
    "print(\"Base44 Scraper initialized\")\n",
    "print(f\"Rate limit: {scraper.rate_limit} seconds between requests\")\n",
    "print(f\"User Agent: {scraper.session.headers['User-Agent'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scraping-showcase",
   "metadata": {},
   "source": [
    "## 1. Scraping Base44 Official Showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scrape-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Base44 official showcase\n",
    "print(\"Starting Base44 showcase scraping...\")\n",
    "showcase_apps = scraper.scrape_base44_showcase()\n",
    "\n",
    "print(f\"Found {len(showcase_apps)} apps from Base44 showcase\")\n",
    "if showcase_apps:\n",
    "    print(\"\\nSample app from showcase:\")\n",
    "    print(json.dumps(showcase_apps[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scraping-product-hunt",
   "metadata": {},
   "source": [
    "## 2. Scraping Product Hunt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scrape-product-hunt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Product Hunt for Base44 applications\n",
    "print(\"Starting Product Hunt search...\")\n",
    "ph_apps = scraper.search_product_hunt(\"Base44\")\n",
    "\n",
    "print(f\"Found {len(ph_apps)} apps from Product Hunt\")\n",
    "if ph_apps:\n",
    "    print(\"\\nSample app from Product Hunt:\")\n",
    "    print(json.dumps(ph_apps[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scraping-social",
   "metadata": {},
   "source": [
    "## 3. Scraping Social Media Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scrape-social",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search social media for Base44 mentions\n",
    "print(\"Starting social media search...\")\n",
    "social_apps = scraper.search_social_media(\"twitter\")\n",
    "\n",
    "print(f\"Found {len(social_apps)} apps from social media\")\n",
    "if social_apps:\n",
    "    print(\"\\nSample app from social media:\")\n",
    "    print(json.dumps(social_apps[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scraping-web",
   "metadata": {},
   "source": [
    "## 4. Web Search for Base44 Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scrape-web",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search web for Base44 app mentions\n",
    "print(\"Starting web search...\")\n",
    "web_apps = scraper.search_web_mentions()\n",
    "\n",
    "print(f\"Found {len(web_apps)} apps from web search\")\n",
    "if web_apps:\n",
    "    print(\"\\nSample app from web search:\")\n",
    "    print(json.dumps(web_apps[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-scrape",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-scrape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete scraping pipeline\n",
    "print(\"Starting comprehensive Base44 application scraping...\")\n",
    "all_apps = scraper.run_full_scrape()\n",
    "\n",
    "print(f\"\\nScraping completed!\")\n",
    "print(f\"Total unique applications found: {len(all_apps)}\")\n",
    "print(f\"Data saved to: data/raw/base44_apps.csv and data/raw/base44_apps.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-exploration",
   "metadata": {},
   "source": [
    "## 6. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-explore-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scraped data\n",
    "try:\n",
    "    apps_df = pd.read_csv('../data/raw/base44_apps.csv')\n",
    "    print(f\"Loaded {len(apps_df)} applications\")\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"\\n=== Dataset Overview ===\")\n",
    "    print(f\"Shape: {apps_df.shape}\")\n",
    "    print(f\"Columns: {list(apps_df.columns)}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\n=== First 5 Applications ===\")\n",
    "    display(apps_df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Data file not found. Please run the scraping cells first.\")\n",
    "    apps_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-quality-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not apps_df.empty:\n",
    "    # Data quality assessment\n",
    "    print(\"=== Data Quality Assessment ===\")\n",
    "    print(\"\\nMissing values:\")\n",
    "    missing_data = apps_df.isnull().sum()\n",
    "    print(missing_data[missing_data > 0])\n",
    "    \n",
    "    print(\"\\nData types:\")\n",
    "    print(apps_df.dtypes)\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = apps_df.duplicated(['name', 'url']).sum()\n",
    "    print(f\"\\nDuplicate entries: {duplicates}\")\n",
    "    \n",
    "    # Source distribution\n",
    "    print(\"\\n=== Source Distribution ===\")\n",
    "    source_counts = apps_df['source'].value_counts()\n",
    "    print(source_counts)\n",
    "    \n",
    "    # Category distribution\n",
    "    print(\"\\n=== Category Distribution ===\")\n",
    "    category_counts = apps_df['category'].value_counts()\n",
    "    print(category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-visualizations",
   "metadata": {},
   "source": [
    "## 7. Initial Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "source-distribution-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not apps_df.empty:\n",
    "    # Create visualizations for initial data exploration\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Base44 Applications - Initial Data Analysis', fontsize=16)\n",
    "    \n",
    "    # Source distribution\n",
    "    source_counts = apps_df['source'].value_counts()\n",
    "    axes[0, 0].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('Distribution by Data Source')\n",
    "    \n",
    "    # Category distribution\n",
    "    category_counts = apps_df['category'].value_counts()\n",
    "    axes[0, 1].bar(category_counts.index, category_counts.values)\n",
    "    axes[0, 1].set_title('Distribution by Application Category')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Industry distribution\n",
    "    if 'industry' in apps_df.columns:\n",
    "        industry_counts = apps_df['industry'].value_counts().head(10)\n",
    "        axes[1, 0].barh(industry_counts.index, industry_counts.values)\n",
    "        axes[1, 0].set_title('Top 10 Industries')\n",
    "    \n",
    "    # Feature count distribution\n",
    "    if 'features' in apps_df.columns:\n",
    "        feature_counts = apps_df['features'].apply(\n",
    "            lambda x: len(str(x).split(',')) if pd.notna(x) else 0\n",
    "        )\n",
    "        axes[1, 1].hist(feature_counts, bins=10, edgecolor='black')\n",
    "        axes[1, 1].set_title('Distribution of Feature Count per App')\n",
    "        axes[1, 1].set_xlabel('Number of Features')\n",
    "        axes[1, 1].set_ylabel('Number of Apps')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-statistics",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not apps_df.empty:\n",
    "    # Generate comprehensive summary statistics\n",
    "    summary_stats = {\n",
    "        'total_applications': len(apps_df),\n",
    "        'unique_sources': apps_df['source'].nunique(),\n",
    "        'unique_categories': apps_df['category'].nunique(),\n",
    "        'unique_industries': apps_df['industry'].nunique() if 'industry' in apps_df.columns else 0,\n",
    "        'apps_with_urls': apps_df['url'].notna().sum(),\n",
    "        'apps_with_descriptions': apps_df['description'].notna().sum(),\n",
    "        'apps_with_features': apps_df['features'].notna().sum(),\n",
    "        'data_collection_date': datetime.now().isoformat(),\n",
    "        'source_breakdown': apps_df['source'].value_counts().to_dict(),\n",
    "        'category_breakdown': apps_df['category'].value_counts().to_dict()\n",
    "    }\n",
    "    \n",
    "    print(\"=== Data Collection Summary ===\")\n",
    "    print(json.dumps(summary_stats, indent=2, default=str))\n",
    "    \n",
    "    # Save summary to file\n",
    "    with open('../data/processed/data_collection_summary.json', 'w') as f:\n",
    "        json.dump(summary_stats, f, indent=2, default=str)\n",
    "    \n",
    "    print(\"\\nSummary saved to: data/processed/data_collection_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-validation",
   "metadata": {},
   "source": [
    "## 9. Data Validation and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-checks",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not apps_df.empty:\n",
    "    print(\"=== Data Validation Results ===\")\n",
    "    \n",
    "    # Check for required fields\n",
    "    required_fields = ['name', 'description', 'category', 'source']\n",
    "    for field in required_fields:\n",
    "        missing_count = apps_df[field].isna().sum()\n",
    "        completeness = (len(apps_df) - missing_count) / len(apps_df) * 100\n",
    "        print(f\"{field}: {completeness:.1f}% complete ({missing_count} missing)\")\n",
    "    \n",
    "    # Check URL validity\n",
    "    valid_urls = apps_df['url'].apply(\n",
    "        lambda x: str(x).startswith(('http://', 'https://')) if pd.notna(x) else False\n",
    "    ).sum()\n",
    "    print(f\"\\nValid URLs: {valid_urls}/{len(apps_df)} ({valid_urls/len(apps_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Check description length\n",
    "    desc_lengths = apps_df['description'].apply(\n",
    "        lambda x: len(str(x)) if pd.notna(x) else 0\n",
    "    )\n",
    "    print(f\"\\nDescription lengths:\")\n",
    "    print(f\"  Average: {desc_lengths.mean():.1f} characters\")\n",
    "    print(f\"  Median: {desc_lengths.median():.1f} characters\")\n",
    "    print(f\"  Range: {desc_lengths.min()}-{desc_lengths.max()} characters\")\n",
    "    \n",
    "    # Feature analysis\n",
    "    if 'features' in apps_df.columns:\n",
    "        feature_counts = apps_df['features'].apply(\n",
    "            lambda x: len(str(x).split(',')) if pd.notna(x) and str(x).strip() else 0\n",
    "        )\n",
    "        print(f\"\\nFeature counts per app:\")\n",
    "        print(f\"  Average: {feature_counts.mean():.1f} features\")\n",
    "        print(f\"  Median: {feature_counts.median():.1f} features\")\n",
    "        print(f\"  Range: {feature_counts.min()}-{feature_counts.max()} features\")\n",
    "    \n",
    "    print(f\"\\n✓ Data validation completed\")\n",
    "    print(f\"✓ Dataset is ready for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The data collection phase is now complete. The collected data includes:\n",
    "\n",
    "1. **Base44 applications** from multiple sources\n",
    "2. **Comprehensive metadata** for each application\n",
    "3. **Quality validation** ensuring data integrity\n",
    "\n",
    "### Files Generated:\n",
    "- `data/raw/base44_apps.csv` - Main dataset\n",
    "- `data/raw/base44_apps.json` - JSON format\n",
    "- `data/processed/data_collection_summary.json` - Collection summary\n",
    "\n",
    "### Next Notebooks:\n",
    "1. **02_app_classification.ipynb** - Classify and categorize applications\n",
    "2. **03_quality_analysis.ipynb** - Evaluate application quality metrics\n",
    "3. **04_visualization_results.ipynb** - Create comprehensive visualizations\n",
    "\n",
    "This data will serve as the foundation for analyzing Base44 as a phenomenon in the no-code development space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}