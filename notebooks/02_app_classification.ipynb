{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Base44 Phenomenon Analysis - Application Classification\n",
    "\n",
    "This notebook performs comprehensive classification and analysis of Base44 applications using machine learning and statistical techniques.\n",
    "\n",
    "## Objectives\n",
    "1. Classify applications by purpose, industry, and user type\n",
    "2. Analyze complexity patterns and feature usage\n",
    "3. Perform sentiment analysis on descriptions\n",
    "4. Identify application clusters and patterns\n",
    "5. Generate insights about Base44 ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from app_analyzer import Base44AppAnalyzer\n",
    "\n",
    "# Import ML libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Analysis started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer\n",
    "analyzer = Base44AppAnalyzer()\n",
    "\n",
    "# Load the scraped data\n",
    "apps_df = analyzer.load_data('../data/raw/base44_apps.csv')\n",
    "\n",
    "if not apps_df.empty:\n",
    "    print(f\"Loaded {len(apps_df)} applications for analysis\")\n",
    "    print(f\"Columns: {list(apps_df.columns)}\")\n",
    "    \n",
    "    # Display basic info\n",
    "    print(\"\\n=== Dataset Overview ===\")\n",
    "    print(apps_df.info())\n",
    "    \n",
    "    # Display sample data\n",
    "    print(\"\\n=== Sample Applications ===\")\n",
    "    display(apps_df[['name', 'category', 'industry', 'description']].head())\n",
    "else:\n",
    "    print(\"No data available. Please run the data collection notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classification-framework",
   "metadata": {},
   "source": [
    "## 2. Classification Framework\n",
    "\n",
    "We'll classify Base44 applications across multiple dimensions:\n",
    "\n",
    "### By Purpose\n",
    "- MVP (Minimum Viable Product)\n",
    "- Internal Tool\n",
    "- Customer Portal\n",
    "- SaaS Replacement\n",
    "- Educational\n",
    "- Personal Project\n",
    "\n",
    "### By Industry\n",
    "- Tech, E-commerce, Education, Healthcare, Finance, Marketing, etc.\n",
    "\n",
    "### By Complexity\n",
    "- Simple (1-3 features)\n",
    "- Medium (4-7 features)\n",
    "- Complex (8+ features)\n",
    "\n",
    "### By User Type\n",
    "- Solo Entrepreneur\n",
    "- Small Business\n",
    "- Enterprise\n",
    "- Student/Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perform-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not apps_df.empty:\n",
    "    # Perform comprehensive analysis\n",
    "    print(\"Starting comprehensive application analysis...\")\n",
    "    analysis_results = analyzer.analyze_all_apps()\n",
    "    \n",
    "    if analysis_results:\n",
    "        print(f\"Analysis completed for {len(analysis_results)} applications\")\n",
    "        \n",
    "        # Convert to DataFrame for easier manipulation\n",
    "        analysis_df = pd.DataFrame([result.__dict__ for result in analysis_results])\n",
    "        \n",
    "        print(\"\\n=== Analysis Results Overview ===\")\n",
    "        display(analysis_df.head())\n",
    "        \n",
    "        # Save analysis results\n",
    "        analyzer.save_analysis_results('../data/processed/app_analysis.csv')\n",
    "        print(\"\\nâœ“ Analysis results saved to data/processed/app_analysis.csv\")\n",
    "    else:\n",
    "        print(\"No analysis results generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purpose-analysis",
   "metadata": {},
   "source": [
    "## 3. Purpose Classification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'analysis_df' in locals() and not analysis_df.empty:\n",
    "    # Analyze purpose distribution\n",
    "    purpose_counts = analysis_df['purpose_category'].value_counts()\n",
    "    \n",
    "    print(\"=== Purpose Distribution ===\")\n",
    "    for purpose, count in purpose_counts.items():\n",
    "        percentage = (count / len(analysis_df)) * 100\n",
    "        print(f\"{purpose}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Pie chart\n",
    "    ax1.pie(purpose_counts.values, labels=purpose_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    ax1.set_title('Distribution of Application Purposes')\n",
    "    \n",
    "    # Bar chart\n",
    "    purpose_counts.plot(kind='bar', ax=ax2, color='skyblue')\n",
    "    ax2.set_title('Application Count by Purpose')\n",
    "    ax2.set_xlabel('Purpose Category')\n",
    "    ax2.set_ylabel('Number of Applications')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Purpose-specific insights\n",
    "    print(\"\\n=== Purpose-Specific Insights ===\")\n",
    "    for purpose in purpose_counts.index:\n",
    "        purpose_apps = analysis_df[analysis_df['purpose_category'] == purpose]\n",
    "        avg_complexity = purpose_apps['complexity_score'].mean()\n",
    "        avg_features = purpose_apps['feature_count'].mean()\n",
    "        print(f\"{purpose}: Avg Complexity = {avg_complexity:.2f}, Avg Features = {avg_features:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industry-analysis",
   "metadata": {},
   "source": [
    "## 4. Industry Classification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'analysis_df' in locals() and not analysis_df.empty:\n",
    "    # Analyze industry distribution\n",
    "    industry_counts = analysis_df['industry_category'].value_counts()\n",
    "    \n",
    "    print(\"=== Industry Distribution ===\")\n",
    "    for industry, count in industry_counts.items():\n",
    "        percentage = (count / len(analysis_df)) * 100\n",
    "        print(f\"{industry}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Horizontal bar chart for better readability\n",
    "    industry_counts.plot(kind='barh', ax=ax, color='lightcoral')\n",
    "    ax.set_title('Base44 Applications by Industry')\n",
    "    ax.set_xlabel('Number of Applications')\n",
    "    ax.set_ylabel('Industry')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(industry_counts.values):\n",
    "        ax.text(v + 0.1, i, str(v), va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Industry-purpose cross-analysis\n",
    "    print(\"\\n=== Industry-Purpose Cross-Analysis ===\")\n",
    "    cross_tab = pd.crosstab(analysis_df['industry_category'], analysis_df['purpose_category'])\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cross_tab, annot=True, fmt='d', cmap='YlOrRd')\n",
    "    plt.title('Application Purpose by Industry (Count)')\n",
    "    plt.xlabel('Purpose Category')\n",
    "    plt.ylabel('Industry Category')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    display(cross_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complexity-analysis",
   "metadata": {},
   "source": [
    "## 5. Complexity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'analysis_df' in locals() and not analysis_df.empty:\n",
    "    # Analyze complexity distribution\n",
    "    print(\"=== Complexity Analysis ===\")\n",
    "    \n",
    "    # Complexity statistics\n",
    "    complexity_stats = analysis_df['complexity_score'].describe()\n",
    "    print(\"Complexity Score Statistics:\")\n",
    "    print(complexity_stats)\n",
    "    \n",
    "    # Categorize by complexity\n",
    "    def categorize_complexity(score):\n",
    "        if score <= 3:\n",
    "            return 'Simple'\n",
    "        elif score <= 6:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'Complex'\n",
    "    \n",
    "    analysis_df['complexity_category'] = analysis_df['complexity_score'].apply(categorize_complexity)\n",
    "    complexity_category_counts = analysis_df['complexity_category'].value_counts()\n",
    "    \n",
    "    print(\"\\nComplexity Categories:\")\n",
    "    for category, count in complexity_category_counts.items():\n",
    "        percentage = (count / len(analysis_df)) * 100\n",
    "        print(f\"{category}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Application Complexity Analysis', fontsize=16)\n",
    "    \n",
    "    # Complexity distribution histogram\n",
    "    axes[0, 0].hist(analysis_df['complexity_score'], bins=15, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].axvline(analysis_df['complexity_score'].mean(), color='red', linestyle='--', \n",
    "                       label=f'Mean: {analysis_df[\"complexity_score\"].mean():.2f}')\n",
    "    axes[0, 0].set_title('Complexity Score Distribution')\n",
    "    axes[0, 0].set_xlabel('Complexity Score')\n",
    "    axes[0, 0].set_ylabel('Number of Applications')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Complexity categories pie chart\n",
    "    axes[0, 1].pie(complexity_category_counts.values, labels=complexity_category_counts.index, \n",
    "                   autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 1].set_title('Complexity Categories')\n",
    "    \n",
    "    # Complexity by purpose\n",
    "    purpose_complexity = analysis_df.groupby('purpose_category')['complexity_score'].mean().sort_values(ascending=False)\n",
    "    purpose_complexity.plot(kind='bar', ax=axes[1, 0], color='lightgreen')\n",
    "    axes[1, 0].set_title('Average Complexity by Purpose')\n",
    "    axes[1, 0].set_xlabel('Purpose Category')\n",
    "    axes[1, 0].set_ylabel('Average Complexity Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Feature count vs complexity scatter\n",
    "    axes[1, 1].scatter(analysis_df['feature_count'], analysis_df['complexity_score'], alpha=0.6)\n",
    "    axes[1, 1].set_title('Feature Count vs Complexity Score')\n",
    "    axes[1, 1].set_xlabel('Number of Features')\n",
    "    axes[1, 1].set_ylabel('Complexity Score')\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    correlation = analysis_df['feature_count'].corr(analysis_df['complexity_score'])\n",
    "    axes[1, 1].text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=axes[1, 1].transAxes, \n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sentiment-analysis",
   "metadata": {},
   "source": [
    "## 6. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-sentiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'analysis_df' in locals() and not analysis_df.empty:\n",
    "    # Analyze sentiment of descriptions\n",
    "    print(\"=== Sentiment Analysis ===\")\n",
    "    \n",
    "    # Sentiment statistics\n",
    "    sentiment_stats = analysis_df['description_sentiment'].describe()\n",
    "    print(\"Sentiment Score Statistics:\")\n",
    "    print(sentiment_stats)\n",
    "    \n",
    "    # Categorize sentiment\n",
    "    def categorize_sentiment(score):\n",
    "        if score > 0.1:\n",
    "            return 'Positive'\n",
    "        elif score < -0.1:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "    \n",
    "    analysis_df['sentiment_category'] = analysis_df['description_sentiment'].apply(categorize_sentiment)\n",
    "    sentiment_counts = analysis_df['sentiment_category'].value_counts()\n",
    "    \n",
    "    print(\"\\nSentiment Categories:\")\n",
    "    for category, count in sentiment_counts.items():\n",
    "        percentage = (count / len(analysis_df)) * 100\n",
    "        print(f\"{category}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle('Sentiment Analysis of Application Descriptions', fontsize=16)\n",
    "    \n",
    "    # Sentiment distribution histogram\n",
    "    axes[0].hist(analysis_df['description_sentiment'], bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(0, color='red', linestyle='--', label='Neutral (0)')\n",
    "    axes[0].axvline(analysis_df['description_sentiment'].mean(), color='green', linestyle='--', \n",
    "                    label=f'Mean: {analysis_df[\"description_sentiment\"].mean():.3f}')\n",
    "    axes[0].set_title('Sentiment Score Distribution')\n",
    "    axes[0].set_xlabel('Sentiment Score')\n",
    "    axes[0].set_ylabel('Number of Applications')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Sentiment categories\n",
    "    colors = ['red', 'gray', 'green']\n",
    "    axes[1].bar(sentiment_counts.index, sentiment_counts.values, color=colors)\n",
    "    axes[1].set_title('Sentiment Categories')\n",
    "    axes[1].set_xlabel('Sentiment Category')\n",
    "    axes[1].set_ylabel('Number of Applications')\n",
    "    \n",
    "    # Sentiment by purpose\n",
    "    sentiment_by_purpose = analysis_df.groupby('purpose_category')['description_sentiment'].mean().sort_values(ascending=False)\n",
    "    sentiment_by_purpose.plot(kind='bar', ax=axes[2], color='lightblue')\n",
    "    axes[2].set_title('Average Sentiment by Purpose')\n",
    "    axes[2].set_xlabel('Purpose Category')\n",
    "    axes[2].set_ylabel('Average Sentiment Score')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    axes[2].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Most positive and negative descriptions\n",
    "    print(\"\\n=== Most Positive Applications ===\")\n",
    "    most_positive = analysis_df.nlargest(3, 'description_sentiment')[['name', 'description_sentiment']]\n",
    "    for idx, row in most_positive.iterrows():\n",
    "        print(f\"{row['name']}: {row['description_sentiment']:.3f}\")\n",
    "    \n",
    "    print(\"\\n=== Most Negative Applications ===\")\n",
    "    most_negative = analysis_df.nsmallest(3, 'description_sentiment')[['name', 'description_sentiment']]\n",
    "    for idx, row in most_negative.iterrows():\n",
    "        print(f\"{row['name']}: {row['description_sentiment']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clustering-analysis",
   "metadata": {},
   "source": [
    "## 7. Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perform-clustering",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not apps_df.empty:\n",
    "    # Perform clustering analysis\n",
    "    print(\"=== Clustering Analysis ===\")\n",
    "    \n",
    "    cluster_analysis = analyzer.perform_clustering_analysis()\n",
    "    \n",
    "    if cluster_analysis:\n",
    "        print(\"\\nCluster Analysis Results:\")\n",
    "        for cluster_id, cluster_info in cluster_analysis.items():\n",
    "            print(f\"\\n{cluster_id.upper()}:\")\n",
    "            print(f\"  Size: {cluster_info['size']} applications\")\n",
    "            print(f\"  Common Purpose: {cluster_info['common_purpose']}\")\n",
    "            print(f\"  Avg Complexity: {cluster_info['avg_complexity']:.2f}\")\n",
    "            print(f\"  Top Keywords: {', '.join(cluster_info['top_keywords'][:5])}\")\n",
    "        \n",
    "        # Visualize clusters if we have cluster labels\n",
    "        if 'cluster' in apps_df.columns:\n",
    "            # Create cluster visualization using PCA\n",
    "            descriptions = apps_df['description'].fillna('').tolist()\n",
    "            vectorizer = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "            tfidf_matrix = vectorizer.fit_transform(descriptions)\n",
    "            \n",
    "            # Reduce dimensionality for visualization\n",
    "            pca = PCA(n_components=2, random_state=42)\n",
    "            coords_2d = pca.fit_transform(tfidf_matrix.toarray())\n",
    "            \n",
    "            # Create scatter plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            scatter = plt.scatter(coords_2d[:, 0], coords_2d[:, 1], \n",
    "                                c=apps_df['cluster'], cmap='viridis', alpha=0.6)\n",
    "            plt.colorbar(scatter)\n",
    "            plt.title('Application Clusters (PCA Visualization)')\n",
    "            plt.xlabel('First Principal Component')\n",
    "            plt.ylabel('Second Principal Component')\n",
    "            \n",
    "            # Add cluster centers\n",
    "            for cluster_id in apps_df['cluster'].unique():\n",
    "                cluster_points = coords_2d[apps_df['cluster'] == cluster_id]\n",
    "                center_x = cluster_points[:, 0].mean()\n",
    "                center_y = cluster_points[:, 1].mean()\n",
    "                plt.annotate(f'C{cluster_id}', (center_x, center_y), \n",
    "                           fontsize=12, fontweight='bold',\n",
    "                           bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No clustering analysis results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-analysis",
   "metadata": {},
   "source": [
    "## 8. Feature Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not apps_df.empty:\n",
    "    # Analyze feature usage patterns\n",
    "    print(\"=== Feature Usage Analysis ===\")\n",
    "    \n",
    "    # Extract all features\n",
    "    all_features = []\n",
    "    for features_str in apps_df['features'].dropna():\n",
    "        features = [f.strip() for f in str(features_str).split(',') if f.strip()]\n",
    "        all_features.extend(features)\n",
    "    \n",
    "    # Count feature frequency\n",
    "    from collections import Counter\n",
    "    feature_counts = Counter(all_features)\n",
    "    \n",
    "    print(f\"Total unique features: {len(feature_counts)}\")\n",
    "    print(f\"Total feature mentions: {sum(feature_counts.values())}\")\n",
    "    \n",
    "    # Top features\n",
    "    top_features = feature_counts.most_common(15)\n",
    "    print(\"\\nTop 15 Most Common Features:\")\n",
    "    for feature, count in top_features:\n",
    "        percentage = (count / len(apps_df)) * 100\n",
    "        print(f\"{feature}: {count} ({percentage:.1f}% of apps)\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Feature Usage Analysis', fontsize=16)\n",
    "    \n",
    "    # Top features bar chart\n",
    "    features, counts = zip(*top_features)\n",
    "    axes[0, 0].barh(features, counts, color='skyblue')\n",
    "    axes[0, 0].set_title('Top 15 Most Common Features')\n",
    "    axes[0, 0].set_xlabel('Number of Applications')\n",
    "    axes[0, 0].invert_yaxis()\n",
    "    \n",
    "    # Feature count distribution\n",
    "    feature_counts_per_app = apps_df['features'].apply(\n",
    "        lambda x: len(str(x).split(',')) if pd.notna(x) and str(x).strip() else 0\n",
    "    )\n",
    "    axes[0, 1].hist(feature_counts_per_app, bins=15, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_title('Distribution of Feature Count per App')\n",
    "    axes[0, 1].set_xlabel('Number of Features')\n",
    "    axes[0, 1].set_ylabel('Number of Applications')\n",
    "    axes[0, 1].axvline(feature_counts_per_app.mean(), color='red', linestyle='--', \n",
    "                       label=f'Mean: {feature_counts_per_app.mean():.1f}')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Feature usage by purpose\n",
    "    if 'analysis_df' in locals() and not analysis_df.empty:\n",
    "        purpose_features = analysis_df.groupby('purpose_category')['feature_count'].mean().sort_values(ascending=False)\n",
    "        purpose_features.plot(kind='bar', ax=axes[1, 0], color='lightcoral')\n",
    "        axes[1, 0].set_title('Average Features by Purpose')\n",
    "        axes[1, 0].set_xlabel('Purpose Category')\n",
    "        axes[1, 0].set_ylabel('Average Number of Features')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Feature diversity (unique features per category)\n",
    "    category_feature_diversity = {}\n",
    "    for category in apps_df['category'].unique():\n",
    "        category_apps = apps_df[apps_df['category'] == category]\n",
    "        category_features = set()\n",
    "        for features_str in category_apps['features'].dropna():\n",
    "            features = [f.strip() for f in str(features_str).split(',') if f.strip()]\n",
    "            category_features.update(features)\n",
    "        category_feature_diversity[category] = len(category_features)\n",
    "    \n",
    "    diversity_df = pd.Series(category_feature_diversity)\n",
    "    diversity_df.plot(kind='bar', ax=axes[1, 1], color='lightgreen')\n",
    "    axes[1, 1].set_title('Feature Diversity by Category')\n",
    "    axes[1, 1].set_xlabel('Category')\n",
    "    axes[1, 1].set_ylabel('Number of Unique Features')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create word cloud of features\n",
    "    if all_features:\n",
    "        feature_text = ' '.join(all_features)\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                             colormap='viridis', max_words=50).generate(feature_text)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Most Common Features in Base44 Applications', fontsize=16, pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-statistics",
   "metadata": {},
   "source": [
    "## 9. Generate Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'analysis_df' in locals() and not analysis_df.empty:\n",
    "    # Generate comprehensive summary statistics\n",
    "    summary_stats = analyzer.generate_summary_statistics()\n",
    "    \n",
    "    print(\"=== COMPREHENSIVE ANALYSIS SUMMARY ===\")\n",
    "    print(json.dumps(summary_stats, indent=2, default=str))\n",
    "    \n",
    "    # Save summary statistics\n",
    "    analyzer.save_summary_stats('../data/processed/summary_statistics.json')\n",
    "    print(\"\\nâœ“ Summary statistics saved to data/processed/summary_statistics.json\")\n",
    "    \n",
    "    # Create summary visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Base44 Applications - Analysis Summary', fontsize=16)\n",
    "    \n",
    "    # Purpose distribution\n",
    "    purpose_dist = summary_stats['purpose_distribution']\n",
    "    axes[0, 0].pie(purpose_dist.values(), labels=purpose_dist.keys(), autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('Purpose Distribution')\n",
    "    \n",
    "    # Industry distribution (top 8)\n",
    "    industry_dist = dict(list(summary_stats['industry_distribution'].items())[:8])\n",
    "    axes[0, 1].bar(industry_dist.keys(), industry_dist.values(), color='lightcoral')\n",
    "    axes[0, 1].set_title('Top Industries')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Complexity statistics\n",
    "    complexity_stats = summary_stats['complexity_stats']\n",
    "    complexity_metrics = ['mean', 'median', 'std', 'min', 'max']\n",
    "    complexity_values = [complexity_stats[metric] for metric in complexity_metrics]\n",
    "    axes[1, 0].bar(complexity_metrics, complexity_values, color='lightgreen')\n",
    "    axes[1, 0].set_title('Complexity Statistics')\n",
    "    axes[1, 0].set_ylabel('Complexity Score')\n",
    "    \n",
    "    # Key metrics summary\n",
    "    key_metrics = {\n",
    "        'Avg Complexity': complexity_stats['mean'],\n",
    "        'Avg Features': summary_stats['feature_stats']['mean'],\n",
    "        'Positive Sentiment %': summary_stats['sentiment_stats']['positive_ratio'] * 100,\n",
    "        'High Market Fit %': summary_stats['market_fit_stats']['high_fit_ratio'] * 100\n",
    "    }\n",
    "    \n",
    "    axes[1, 1].bar(key_metrics.keys(), key_metrics.values(), color='gold')\n",
    "    axes[1, 1].set_title('Key Metrics Summary')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insights-findings",
   "metadata": {},
   "source": [
    "## 10. Key Insights and Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-insights",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'summary_stats' in locals():\n",
    "    print(\"=== KEY INSIGHTS FROM BASE44 ANALYSIS ===\")\n",
    "    print()\n",
    "    \n",
    "    # Most popular application types\n",
    "    top_purpose = max(summary_stats['purpose_distribution'], key=summary_stats['purpose_distribution'].get)\n",
    "    top_industry = max(summary_stats['industry_distribution'], key=summary_stats['industry_distribution'].get)\n",
    "    \n",
    "    print(f\"ðŸ“Š DEMOGRAPHICS:\")\n",
    "    print(f\"   â€¢ Most common purpose: {top_purpose} ({summary_stats['purpose_distribution'][top_purpose]} apps)\")\n",
    "    print(f\"   â€¢ Most common industry: {top_industry} ({summary_stats['industry_distribution'][top_industry]} apps)\")\n",
    "    print(f\"   â€¢ Total applications analyzed: {summary_stats['total_apps']}\")\n",
    "    print()\n",
    "    \n",
    "    # Complexity insights\n",
    "    avg_complexity = summary_stats['complexity_stats']['mean']\n",
    "    complexity_level = 'High' if avg_complexity > 6 else 'Medium' if avg_complexity > 3 else 'Low'\n",
    "    \n",
    "    print(f\"ðŸ”§ COMPLEXITY & FEATURES:\")\n",
    "    print(f\"   â€¢ Average complexity: {avg_complexity:.2f}/10 ({complexity_level})\")\n",
    "    print(f\"   â€¢ Average features per app: {summary_stats['feature_stats']['mean']:.1f}\")\n",
    "    print(f\"   â€¢ Most feature-rich app: {summary_stats['feature_stats']['max']} features\")\n",
    "    print()\n",
    "    \n",
    "    # Sentiment insights\n",
    "    positive_ratio = summary_stats['sentiment_stats']['positive_ratio'] * 100\n",
    "    negative_ratio = summary_stats['sentiment_stats']['negative_ratio'] * 100\n",
    "    \n",
    "    print(f\"ðŸ’­ SENTIMENT & RECEPTION:\")\n",
    "    print(f\"   â€¢ Positive descriptions: {positive_ratio:.1f}%\")\n",
    "    print(f\"   â€¢ Negative descriptions: {negative_ratio:.1f}%\")\n",
    "    print(f\"   â€¢ Average sentiment: {summary_stats['sentiment_stats']['mean']:.3f}\")\n",
    "    print()\n",
    "    \n",
    "    # Market fit insights\n",
    "    high_market_fit = summary_stats['market_fit_stats']['high_fit_ratio'] * 100\n",
    "    avg_market_fit = summary_stats['market_fit_stats']['mean']\n",
    "    \n",
    "    print(f\"ðŸŽ¯ MARKET FIT & SUCCESS:\")\n",
    "    print(f\"   â€¢ High market fit apps: {high_market_fit:.1f}%\")\n",
    "    print(f\"   â€¢ Average market fit score: {avg_market_fit:.2f}/10\")\n",
    "    print()\n",
    "    \n",
    "    # Innovation insights\n",
    "    high_innovation = summary_stats['innovation_stats']['high_innovation_ratio'] * 100\n",
    "    avg_innovation = summary_stats['innovation_stats']['mean']\n",
    "    \n",
    "    print(f\"ðŸš€ INNOVATION:\")\n",
    "    print(f\"   â€¢ Highly innovative apps: {high_innovation:.1f}%\")\n",
    "    print(f\"   â€¢ Average innovation score: {avg_innovation:.2f}/10\")\n",
    "    print()\n",
    "    \n",
    "    # Generate recommendations\n",
    "    print(f\"ðŸ“ˆ RECOMMENDATIONS FOR BASE44 USERS:\")\n",
    "    \n",
    "    if top_purpose == 'Internal Tool':\n",
    "        print(f\"   â€¢ Focus on internal tools - they're the most popular use case\")\n",
    "    elif top_purpose == 'SaaS Replacement':\n",
    "        print(f\"   â€¢ SaaS replacement is a key opportunity - cost savings are important\")\n",
    "    \n",
    "    if avg_complexity < 5:\n",
    "        print(f\"   â€¢ Most apps are relatively simple - don't over-engineer\")\n",
    "    else:\n",
    "        print(f\"   â€¢ Users are building complex applications - advanced features are valued\")\n",
    "    \n",
    "    if positive_ratio > 70:\n",
    "        print(f\"   â€¢ High positive sentiment indicates strong user satisfaction\")\n",
    "    \n",
    "    if high_market_fit > 50:\n",
    "        print(f\"   â€¢ Good market fit suggests Base44 is solving real problems\")\n",
    "    \n",
    "    print()\n",
    "    print(\"âœ… Analysis completed successfully!\")\n",
    "    print(\"   Next: Run quality analysis and visualization notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This comprehensive classification analysis of Base44 applications reveals several key patterns:\n",
    "\n",
    "### Key Findings:\n",
    "1. **Application Purposes**: Distribution across MVP, Internal Tools, Customer Portals, etc.\n",
    "2. **Industry Adoption**: Identifying which industries are most active\n",
    "3. **Complexity Patterns**: Understanding the sophistication of Base44 applications\n",
    "4. **Sentiment Analysis**: Overall positive reception of the platform\n",
    "5. **Feature Usage**: Most common features and patterns\n",
    "\n",
    "### Files Generated:\n",
    "- `data/processed/app_analysis.csv` - Detailed analysis results\n",
    "- `data/processed/summary_statistics.json` - Comprehensive summary\n",
    "\n",
    "### Next Steps:\n",
    "1. **Quality Analysis** - Evaluate application quality metrics\n",
    "2. **Visualization** - Create comprehensive charts and graphs\n",
    "3. **Academic Paper** - Synthesize findings into research paper\n",
    "\n",
    "This analysis provides a solid foundation for understanding Base44 as a phenomenon in the no-code development space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}