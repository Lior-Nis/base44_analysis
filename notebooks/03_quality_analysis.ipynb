{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Base44 Phenomenon Analysis - Quality Analysis\n",
    "\n",
    "This notebook performs comprehensive quality evaluation of Base44 applications using multiple metrics and analysis techniques.\n",
    "\n",
    "## Quality Metrics Framework\n",
    "\n",
    "We evaluate applications across six key dimensions:\n",
    "\n",
    "1. **Completeness Score** (0-10): Feature completeness vs stated purpose\n",
    "2. **Professional Score** (0-10): UI polish, branding, custom domain\n",
    "3. **Adoption Score** (0-10): Public mentions, testimonials, social shares\n",
    "4. **Replacement Success** (0-10): Cost savings and feature parity for SaaS replacements\n",
    "5. **Time-to-Market** (0-10): Development speed indicators\n",
    "6. **Longevity Score** (0-10): Active maintenance and accessibility\n",
    "\n",
    "## Objectives\n",
    "1. Evaluate quality metrics for all Base44 applications\n",
    "2. Identify patterns in application quality\n",
    "3. Analyze quality by purpose, industry, and complexity\n",
    "4. Generate insights about successful Base44 applications\n",
    "5. Create quality-based recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from quality_metrics import Base44QualityEvaluator\n",
    "\n",
    "# Import statistical libraries\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Quality analysis started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## 1. Load Data and Initialize Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the quality evaluator\n",
    "evaluator = Base44QualityEvaluator(rate_limit=1.0)\n",
    "\n",
    "# Load existing analysis data\n",
    "try:\n",
    "    apps_df = pd.read_csv('../data/raw/base44_apps.csv')\n",
    "    analysis_df = pd.read_csv('../data/processed/app_analysis.csv')\n",
    "    \n",
    "    print(f\"Loaded {len(apps_df)} applications and {len(analysis_df)} analysis records\")\n",
    "    \n",
    "    # Display data overview\n",
    "    print(\"\\n=== Available Data ===\")\n",
    "    print(f\"Apps DataFrame: {apps_df.shape}\")\n",
    "    print(f\"Analysis DataFrame: {analysis_df.shape}\")\n",
    "    \n",
    "    # Sample of apps data\n",
    "    print(\"\\n=== Sample Application Data ===\")\n",
    "    display(apps_df[['name', 'url', 'category', 'description']].head(3))\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Data file not found: {e}\")\n",
    "    print(\"Please run the previous notebooks first to generate the required data.\")\n",
    "    apps_df = pd.DataFrame()\n",
    "    analysis_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-evaluation",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-quality-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not apps_df.empty:\n",
    "    print(\"=== Starting Comprehensive Quality Evaluation ===\")\n",
    "    print(\"This may take a few minutes as we analyze each application...\")\n",
    "    \n",
    "    # Run quality evaluation for all applications\n",
    "    quality_results = evaluator.evaluate_all_apps('../data/raw/base44_apps.csv')\n",
    "    \n",
    "    if quality_results:\n",
    "        print(f\"\\n‚úì Quality evaluation completed for {len(quality_results)} applications\")\n",
    "        \n",
    "        # Convert to DataFrame for analysis\n",
    "        quality_df = pd.DataFrame([result.__dict__ for result in quality_results])\n",
    "        \n",
    "        print(\"\\n=== Quality Metrics Overview ===\")\n",
    "        display(quality_df.head())\n",
    "        \n",
    "        # Save quality results\n",
    "        evaluator.save_quality_results('../data/processed/quality_metrics.csv')\n",
    "        print(\"\\n‚úì Quality metrics saved to data/processed/quality_metrics.csv\")\n",
    "        \n",
    "        # Display basic statistics\n",
    "        print(\"\\n=== Quality Score Statistics ===\")\n",
    "        quality_columns = ['completeness_score', 'professional_score', 'adoption_score', \n",
    "                          'replacement_success_score', 'time_to_market_score', 'longevity_score', \n",
    "                          'overall_quality_score']\n",
    "        \n",
    "        display(quality_df[quality_columns].describe())\n",
    "    else:\n",
    "        print(\"No quality evaluation results generated\")\n",
    "        quality_df = pd.DataFrame()\n",
    "else:\n",
    "    print(\"No application data available for quality evaluation\")\n",
    "    quality_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-distribution",
   "metadata": {},
   "source": [
    "## 3. Quality Score Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-quality-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not quality_df.empty:\n",
    "    print(\"=== Quality Score Distribution Analysis ===\")\n",
    "    \n",
    "    # Create comprehensive quality distribution visualization\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    fig.suptitle('Base44 Applications - Quality Metrics Distribution', fontsize=16)\n",
    "    \n",
    "    quality_metrics = [\n",
    "        ('completeness_score', 'Completeness Score'),\n",
    "        ('professional_score', 'Professional Score'),\n",
    "        ('adoption_score', 'Adoption Score'),\n",
    "        ('replacement_success_score', 'Replacement Success'),\n",
    "        ('time_to_market_score', 'Time to Market'),\n",
    "        ('longevity_score', 'Longevity Score'),\n",
    "        ('overall_quality_score', 'Overall Quality')\n",
    "    ]\n",
    "    \n",
    "    # Plot distribution for each metric\n",
    "    for i, (metric, title) in enumerate(quality_metrics):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        \n",
    "        # Histogram\n",
    "        axes[row, col].hist(quality_df[metric], bins=15, alpha=0.7, edgecolor='black')\n",
    "        axes[row, col].axvline(quality_df[metric].mean(), color='red', linestyle='--', \n",
    "                               label=f'Mean: {quality_df[metric].mean():.2f}')\n",
    "        axes[row, col].axvline(quality_df[metric].median(), color='green', linestyle='--', \n",
    "                               label=f'Median: {quality_df[metric].median():.2f}')\n",
    "        axes[row, col].set_title(title)\n",
    "        axes[row, col].set_xlabel('Score')\n",
    "        axes[row, col].set_ylabel('Frequency')\n",
    "        axes[row, col].legend(fontsize=8)\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot comparison (bottom middle)\n",
    "    metric_names = [title for _, title in quality_metrics]\n",
    "    metric_data = [quality_df[metric] for metric, _ in quality_metrics]\n",
    "    \n",
    "    axes[2, 1].boxplot(metric_data, labels=metric_names)\n",
    "    axes[2, 1].set_title('Quality Metrics Comparison')\n",
    "    axes[2, 1].set_ylabel('Score')\n",
    "    axes[2, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Correlation heatmap (bottom right)\n",
    "    correlation_matrix = quality_df[['completeness_score', 'professional_score', 'adoption_score', \n",
    "                                   'replacement_success_score', 'time_to_market_score', \n",
    "                                   'longevity_score', 'overall_quality_score']].corr()\n",
    "    \n",
    "    im = axes[2, 2].imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "    axes[2, 2].set_xticks(range(len(correlation_matrix.columns)))\n",
    "    axes[2, 2].set_yticks(range(len(correlation_matrix.columns)))\n",
    "    axes[2, 2].set_xticklabels(['Comp', 'Prof', 'Adopt', 'Repl', 'TTM', 'Long', 'Overall'], rotation=45)\n",
    "    axes[2, 2].set_yticklabels(['Comp', 'Prof', 'Adopt', 'Repl', 'TTM', 'Long', 'Overall'])\n",
    "    axes[2, 2].set_title('Quality Metrics Correlation')\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(len(correlation_matrix)):\n",
    "        for j in range(len(correlation_matrix)):\n",
    "            axes[2, 2].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', \n",
    "                           ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print quality insights\n",
    "    print(\"\\n=== Quality Distribution Insights ===\")\n",
    "    \n",
    "    for metric, title in quality_metrics:\n",
    "        mean_score = quality_df[metric].mean()\n",
    "        std_score = quality_df[metric].std()\n",
    "        high_quality_pct = (quality_df[metric] >= 7).sum() / len(quality_df) * 100\n",
    "        \n",
    "        print(f\"{title}:\")\n",
    "        print(f\"  Mean: {mean_score:.2f} ¬± {std_score:.2f}\")\n",
    "        print(f\"  High quality (‚â•7): {high_quality_pct:.1f}%\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-by-category",
   "metadata": {},
   "source": [
    "## 4. Quality Analysis by Application Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-quality-by-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not quality_df.empty and not analysis_df.empty:\n",
    "    print(\"=== Quality Analysis by Application Category ===\")\n",
    "    \n",
    "    # Merge quality and analysis data\n",
    "    merged_df = pd.merge(quality_df, analysis_df, left_on='app_name', right_on='name', how='inner')\n",
    "    \n",
    "    print(f\"Merged data for {len(merged_df)} applications\")\n",
    "    \n",
    "    # Quality by purpose category\n",
    "    purpose_quality = merged_df.groupby('purpose_category').agg({\n",
    "        'overall_quality_score': ['mean', 'std', 'count'],\n",
    "        'completeness_score': 'mean',\n",
    "        'professional_score': 'mean',\n",
    "        'adoption_score': 'mean',\n",
    "        'longevity_score': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\n=== Quality by Purpose Category ===\")\n",
    "    display(purpose_quality)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Quality Analysis by Application Categories', fontsize=16)\n",
    "    \n",
    "    # Overall quality by purpose\n",
    "    purpose_means = merged_df.groupby('purpose_category')['overall_quality_score'].mean().sort_values(ascending=False)\n",
    "    purpose_means.plot(kind='bar', ax=axes[0, 0], color='skyblue')\n",
    "    axes[0, 0].set_title('Average Overall Quality by Purpose')\n",
    "    axes[0, 0].set_xlabel('Purpose Category')\n",
    "    axes[0, 0].set_ylabel('Average Quality Score')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Quality by industry\n",
    "    industry_quality = merged_df.groupby('industry_category')['overall_quality_score'].mean().sort_values(ascending=False).head(8)\n",
    "    industry_quality.plot(kind='barh', ax=axes[0, 1], color='lightcoral')\n",
    "    axes[0, 1].set_title('Average Quality by Industry (Top 8)')\n",
    "    axes[0, 1].set_xlabel('Average Quality Score')\n",
    "    axes[0, 1].set_ylabel('Industry')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot: Quality distribution by purpose\n",
    "    purpose_categories = merged_df['purpose_category'].unique()\n",
    "    quality_by_purpose = [merged_df[merged_df['purpose_category'] == cat]['overall_quality_score'] \n",
    "                         for cat in purpose_categories]\n",
    "    \n",
    "    axes[1, 0].boxplot(quality_by_purpose, labels=purpose_categories)\n",
    "    axes[1, 0].set_title('Quality Distribution by Purpose')\n",
    "    axes[1, 0].set_xlabel('Purpose Category')\n",
    "    axes[1, 0].set_ylabel('Overall Quality Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Complexity vs Quality scatter by purpose\n",
    "    for purpose in purpose_categories:\n",
    "        purpose_data = merged_df[merged_df['purpose_category'] == purpose]\n",
    "        axes[1, 1].scatter(purpose_data['complexity_score'], purpose_data['overall_quality_score'], \n",
    "                          label=purpose, alpha=0.7)\n",
    "    \n",
    "    axes[1, 1].set_title('Complexity vs Quality by Purpose')\n",
    "    axes[1, 1].set_xlabel('Complexity Score')\n",
    "    axes[1, 1].set_ylabel('Overall Quality Score')\n",
    "    axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(\"\\n=== Statistical Analysis ===\")\n",
    "    \n",
    "    # ANOVA test for quality differences between purposes\n",
    "    purpose_groups = [merged_df[merged_df['purpose_category'] == cat]['overall_quality_score'] \n",
    "                     for cat in purpose_categories]\n",
    "    \n",
    "    f_stat, p_value = stats.f_oneway(*purpose_groups)\n",
    "    print(f\"ANOVA test for quality differences between purposes:\")\n",
    "    print(f\"F-statistic: {f_stat:.3f}, p-value: {p_value:.3f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"Significant differences in quality between purpose categories (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"No significant differences in quality between purpose categories (p ‚â• 0.05)\")\n",
    "        \n",
    "    # Correlation between complexity and quality\n",
    "    correlation = merged_df['complexity_score'].corr(merged_df['overall_quality_score'])\n",
    "    print(f\"\\nCorrelation between complexity and quality: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "top-performers",
   "metadata": {},
   "source": [
    "## 5. Top Performing Applications Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-top-performers",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not quality_df.empty:\n",
    "    print(\"=== Top Performing Applications Analysis ===\")\n",
    "    \n",
    "    # Define high-quality threshold\n",
    "    high_quality_threshold = 7.0\n",
    "    \n",
    "    # Top applications by overall quality\n",
    "    top_overall = quality_df.nlargest(10, 'overall_quality_score')\n",
    "    print(f\"\\n=== Top 10 Applications by Overall Quality ===\")\n",
    "    display(top_overall[['app_name', 'overall_quality_score', 'completeness_score', \n",
    "                        'professional_score', 'adoption_score']].round(2))\n",
    "    \n",
    "    # High-quality applications analysis\n",
    "    high_quality_apps = quality_df[quality_df['overall_quality_score'] >= high_quality_threshold]\n",
    "    high_quality_percentage = len(high_quality_apps) / len(quality_df) * 100\n",
    "    \n",
    "    print(f\"\\n=== High-Quality Applications (‚â•{high_quality_threshold}) ===\")\n",
    "    print(f\"Count: {len(high_quality_apps)} out of {len(quality_df)} ({high_quality_percentage:.1f}%)\")\n",
    "    \n",
    "    if len(high_quality_apps) > 0:\n",
    "        # Characteristics of high-quality apps\n",
    "        high_quality_stats = high_quality_apps[[\n",
    "            'completeness_score', 'professional_score', 'adoption_score',\n",
    "            'replacement_success_score', 'time_to_market_score', 'longevity_score'\n",
    "        ]].mean()\n",
    "        \n",
    "        print(\"\\nAverage scores for high-quality applications:\")\n",
    "        for metric, score in high_quality_stats.items():\n",
    "            print(f\"  {metric.replace('_', ' ').title()}: {score:.2f}\")\n",
    "    \n",
    "    # Best performers by individual metrics\n",
    "    print(\"\\n=== Best Performers by Individual Metrics ===\")\n",
    "    \n",
    "    metrics_to_analyze = [\n",
    "        ('completeness_score', 'Most Complete'),\n",
    "        ('professional_score', 'Most Professional'),\n",
    "        ('adoption_score', 'Most Adopted'),\n",
    "        ('time_to_market_score', 'Fastest Development'),\n",
    "        ('longevity_score', 'Most Durable')\n",
    "    ]\n",
    "    \n",
    "    for metric, title in metrics_to_analyze:\n",
    "        top_metric = quality_df.nlargest(3, metric)\n",
    "        print(f\"\\n{title}:\")\n",
    "        for idx, row in top_metric.iterrows():\n",
    "            print(f\"  {row['app_name']}: {row[metric]:.2f}\")\n",
    "    \n",
    "    # Create radar chart for top 5 applications\n",
    "    import matplotlib.pyplot as plt\n",
    "    from math import pi\n",
    "    \n",
    "    top_5_apps = quality_df.nlargest(5, 'overall_quality_score')\n",
    "    \n",
    "    # Prepare data for radar chart\n",
    "    metrics = ['completeness_score', 'professional_score', 'adoption_score', \n",
    "              'replacement_success_score', 'time_to_market_score', 'longevity_score']\n",
    "    metric_labels = ['Completeness', 'Professional', 'Adoption', 'Replacement', 'Time to Market', 'Longevity']\n",
    "    \n",
    "    # Create radar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 8), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Angles for each metric\n",
    "    angles = [n / float(len(metrics)) * 2 * pi for n in range(len(metrics))]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    # Plot each app\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    for i, (idx, app) in enumerate(top_5_apps.iterrows()):\n",
    "        values = [app[metric] for metric in metrics]\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=app['app_name'][:20], color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[i])\n",
    "    \n",
    "    # Customize the chart\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metric_labels)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.set_title('Top 5 Applications - Quality Metrics Comparison', size=16, pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-patterns",
   "metadata": {},
   "source": [
    "## 6. Quality Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-quality-patterns",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not quality_df.empty:\n",
    "    print(\"=== Quality Pattern Analysis ===\")\n",
    "    \n",
    "    # Identify quality clusters using K-means\n",
    "    quality_features = ['completeness_score', 'professional_score', 'adoption_score', \n",
    "                       'replacement_success_score', 'time_to_market_score', 'longevity_score']\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(quality_df[quality_features])\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    n_clusters = 4\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    quality_clusters = kmeans.fit_predict(scaled_features)\n",
    "    \n",
    "    # Add cluster labels to dataframe\n",
    "    quality_df['quality_cluster'] = quality_clusters\n",
    "    \n",
    "    # Analyze clusters\n",
    "    print(f\"\\n=== Quality Clusters (K={n_clusters}) ===\")\n",
    "    \n",
    "    cluster_analysis = {}\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_apps = quality_df[quality_df['quality_cluster'] == cluster_id]\n",
    "        cluster_means = cluster_apps[quality_features + ['overall_quality_score']].mean()\n",
    "        \n",
    "        print(f\"\\nCluster {cluster_id} ({len(cluster_apps)} apps):\")\n",
    "        print(f\"  Average Overall Quality: {cluster_means['overall_quality_score']:.2f}\")\n",
    "        print(f\"  Strongest Metric: {cluster_means[quality_features].idxmax().replace('_', ' ').title()}\")\n",
    "        print(f\"  Weakest Metric: {cluster_means[quality_features].idxmin().replace('_', ' ').title()}\")\n",
    "        \n",
    "        cluster_analysis[cluster_id] = {\n",
    "            'size': len(cluster_apps),\n",
    "            'avg_quality': cluster_means['overall_quality_score'],\n",
    "            'characteristics': cluster_means[quality_features].to_dict()\n",
    "        }\n",
    "    \n",
    "    # Visualize clusters\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Quality Pattern Analysis', fontsize=16)\n",
    "    \n",
    "    # PCA visualization of clusters\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    pca_features = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    scatter = axes[0, 0].scatter(pca_features[:, 0], pca_features[:, 1], \n",
    "                                c=quality_clusters, cmap='viridis', alpha=0.7)\n",
    "    axes[0, 0].set_title('Quality Clusters (PCA Visualization)')\n",
    "    axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[0, 0])\n",
    "    \n",
    "    # Cluster size distribution\n",
    "    cluster_sizes = [cluster_analysis[i]['size'] for i in range(n_clusters)]\n",
    "    cluster_labels = [f'Cluster {i}' for i in range(n_clusters)]\n",
    "    \n",
    "    axes[0, 1].pie(cluster_sizes, labels=cluster_labels, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 1].set_title('Cluster Size Distribution')\n",
    "    \n",
    "    # Average quality by cluster\n",
    "    cluster_qualities = [cluster_analysis[i]['avg_quality'] for i in range(n_clusters)]\n",
    "    \n",
    "    axes[1, 0].bar(cluster_labels, cluster_qualities, color=['red', 'orange', 'green', 'blue'])\n",
    "    axes[1, 0].set_title('Average Quality by Cluster')\n",
    "    axes[1, 0].set_ylabel('Average Overall Quality Score')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Heatmap of cluster characteristics\n",
    "    cluster_heatmap_data = []\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_heatmap_data.append(list(cluster_analysis[cluster_id]['characteristics'].values()))\n",
    "    \n",
    "    im = axes[1, 1].imshow(cluster_heatmap_data, cmap='RdYlGn', aspect='auto')\n",
    "    axes[1, 1].set_xticks(range(len(quality_features)))\n",
    "    axes[1, 1].set_yticks(range(n_clusters))\n",
    "    axes[1, 1].set_xticklabels([f.replace('_', '\\n').title() for f in quality_features], rotation=45)\n",
    "    axes[1, 1].set_yticklabels([f'Cluster {i}' for i in range(n_clusters)])\n",
    "    axes[1, 1].set_title('Cluster Characteristics Heatmap')\n",
    "    \n",
    "    # Add values to heatmap\n",
    "    for i in range(n_clusters):\n",
    "        for j in range(len(quality_features)):\n",
    "            axes[1, 1].text(j, i, f'{cluster_heatmap_data[i][j]:.1f}', \n",
    "                           ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[1, 1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Quality success factors analysis\n",
    "    print(\"\\n=== Quality Success Factors ===\")\n",
    "    \n",
    "    # Correlations with overall quality\n",
    "    correlations = quality_df[quality_features].corrwith(quality_df['overall_quality_score']).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Correlation with Overall Quality Score:\")\n",
    "    for metric, correlation in correlations.items():\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {correlation:.3f}\")\n",
    "    \n",
    "    # Identify the most important quality factors\n",
    "    print(f\"\\nMost important quality factor: {correlations.index[0].replace('_', ' ').title()}\")\n",
    "    print(f\"Least important quality factor: {correlations.index[-1].replace('_', ' ').title()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-report",
   "metadata": {},
   "source": [
    "## 7. Generate Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-quality-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not quality_df.empty:\n",
    "    print(\"=== Generating Comprehensive Quality Report ===\")\n",
    "    \n",
    "    # Generate and save quality report\n",
    "    quality_report = evaluator.generate_quality_report()\n",
    "    evaluator.save_quality_report('../data/processed/quality_report.json')\n",
    "    \n",
    "    if quality_report:\n",
    "        print(\"\\n=== QUALITY ANALYSIS REPORT ===\")\n",
    "        print(json.dumps(quality_report, indent=2, default=str))\n",
    "        \n",
    "        print(\"\\n‚úì Quality report saved to data/processed/quality_report.json\")\n",
    "        \n",
    "        # Extract key insights\n",
    "        evaluation_summary = quality_report.get('evaluation_summary', {})\n",
    "        quality_distributions = quality_report.get('quality_distributions', {})\n",
    "        top_apps = quality_report.get('top_apps', {})\n",
    "        insights = quality_report.get('quality_insights', {})\n",
    "        \n",
    "        print(\"\\n=== KEY QUALITY INSIGHTS ===\")\n",
    "        print(f\"üìä Total Applications Evaluated: {evaluation_summary.get('total_apps_evaluated', 0)}\")\n",
    "        print(f\"üìà Average Overall Quality: {evaluation_summary.get('average_overall_quality', 0):.2f}/10\")\n",
    "        \n",
    "        overall_dist = quality_distributions.get('overall_quality', {})\n",
    "        print(f\"üéØ Quality Range: {overall_dist.get('min', 0):.1f} - {overall_dist.get('max', 0):.1f}\")\n",
    "        print(f\"üìè Quality Std Dev: {overall_dist.get('std', 0):.2f}\")\n",
    "        \n",
    "        completeness_dist = quality_distributions.get('completeness', {})\n",
    "        professional_dist = quality_distributions.get('professional', {})\n",
    "        adoption_dist = quality_distributions.get('adoption', {})\n",
    "        \n",
    "        print(f\"\\nüèÜ HIGH PERFORMANCE METRICS:\")\n",
    "        print(f\"   ‚Ä¢ High Completeness: {completeness_dist.get('high_completeness_ratio', 0)*100:.1f}% of apps\")\n",
    "        print(f\"   ‚Ä¢ Professional Quality: {professional_dist.get('professional_ratio', 0)*100:.1f}% of apps\")\n",
    "        print(f\"   ‚Ä¢ High Adoption: {adoption_dist.get('high_adoption_ratio', 0)*100:.1f}% of apps\")\n",
    "        \n",
    "        print(f\"\\nü•á TOP PERFORMING APPLICATIONS:\")\n",
    "        top_overall = top_apps.get('highest_overall_quality', [])\n",
    "        for i, app in enumerate(top_overall[:3], 1):\n",
    "            print(f\"   {i}. {app.get('app_name', 'Unknown')}: {app.get('overall_quality_score', 0):.2f}\")\n",
    "        \n",
    "        print(f\"\\nüí° INSIGHTS:\")\n",
    "        for insight_key, insight_text in insights.items():\n",
    "            print(f\"   ‚Ä¢ {insight_text}\")\n",
    "    else:\n",
    "        print(\"No quality report generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recommendations",
   "metadata": {},
   "source": [
    "## 8. Quality-Based Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not quality_df.empty and 'quality_report' in locals():\n",
    "    print(\"=== QUALITY-BASED RECOMMENDATIONS ===\")\n",
    "    \n",
    "    # Analyze quality patterns for recommendations\n",
    "    avg_overall_quality = quality_df['overall_quality_score'].mean()\n",
    "    high_quality_apps = quality_df[quality_df['overall_quality_score'] >= 7]\n",
    "    high_quality_percentage = len(high_quality_apps) / len(quality_df) * 100\n",
    "    \n",
    "    # Calculate average scores for each metric\n",
    "    avg_completeness = quality_df['completeness_score'].mean()\n",
    "    avg_professional = quality_df['professional_score'].mean()\n",
    "    avg_adoption = quality_df['adoption_score'].mean()\n",
    "    avg_ttm = quality_df['time_to_market_score'].mean()\n",
    "    avg_longevity = quality_df['longevity_score'].mean()\n",
    "    \n",
    "    print(f\"\\nüéØ FOR BASE44 PLATFORM USERS:\")\n",
    "    \n",
    "    # Recommendations based on quality patterns\n",
    "    if avg_completeness < 6:\n",
    "        print(f\"   ‚Ä¢ Focus on FEATURE COMPLETENESS - avg score is only {avg_completeness:.1f}/10\")\n",
    "        print(f\"     ‚Üí Plan your app features thoroughly before building\")\n",
    "        print(f\"     ‚Üí Use the classification framework to ensure key features for your app type\")\n",
    "    \n",
    "    if avg_professional < 6:\n",
    "        print(f\"   ‚Ä¢ Improve PROFESSIONAL PRESENTATION - avg score is {avg_professional:.1f}/10\")\n",
    "        print(f\"     ‚Üí Consider custom domains instead of base44.app subdomains\")\n",
    "        print(f\"     ‚Üí Invest in UI/UX polish and branding\")\n",
    "    \n",
    "    if avg_adoption < 5:\n",
    "        print(f\"   ‚Ä¢ Increase ADOPTION & VISIBILITY - avg score is {avg_adoption:.1f}/10\")\n",
    "        print(f\"     ‚Üí Share your apps on social media and Product Hunt\")\n",
    "        print(f\"     ‚Üí Collect and display user testimonials\")\n",
    "    \n",
    "    if avg_ttm > 7:\n",
    "        print(f\"   ‚Ä¢ Base44 enables FAST DEVELOPMENT - avg time-to-market score: {avg_ttm:.1f}/10\")\n",
    "        print(f\"     ‚Üí Leverage this speed advantage for rapid prototyping\")\n",
    "        print(f\"     ‚Üí Use Base44 for MVPs and proof-of-concepts\")\n",
    "    \n",
    "    print(f\"\\nüè¢ FOR BASE44 PLATFORM:\")\n",
    "    \n",
    "    if high_quality_percentage < 30:\n",
    "        print(f\"   ‚Ä¢ Only {high_quality_percentage:.1f}% of apps achieve high quality (‚â•7.0)\")\n",
    "        print(f\"     ‚Üí Provide better templates and best practices\")\n",
    "        print(f\"     ‚Üí Offer quality assessment tools\")\n",
    "    \n",
    "    # Identify weakest quality areas\n",
    "    quality_averages = {\n",
    "        'Completeness': avg_completeness,\n",
    "        'Professional': avg_professional,\n",
    "        'Adoption': avg_adoption,\n",
    "        'Time to Market': avg_ttm,\n",
    "        'Longevity': avg_longevity\n",
    "    }\n",
    "    \n",
    "    weakest_area = min(quality_averages, key=quality_averages.get)\n",
    "    strongest_area = max(quality_averages, key=quality_averages.get)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Weakest area: {weakest_area} ({quality_averages[weakest_area]:.1f}/10)\")\n",
    "    print(f\"     ‚Üí Focus platform improvements on this area\")\n",
    "    print(f\"   ‚Ä¢ Strongest area: {strongest_area} ({quality_averages[strongest_area]:.1f}/10)\")\n",
    "    print(f\"     ‚Üí Highlight this advantage in marketing\")\n",
    "    \n",
    "    print(f\"\\nüìä FOR RESEARCHERS & ANALYSTS:\")\n",
    "    print(f\"   ‚Ä¢ Average overall quality: {avg_overall_quality:.2f}/10 indicates {'good' if avg_overall_quality >= 6 else 'moderate'} ecosystem maturity\")\n",
    "    print(f\"   ‚Ä¢ Quality distribution shows {'concentrated' if quality_df['overall_quality_score'].std() < 1.5 else 'diverse'} application quality\")\n",
    "    \n",
    "    if 'merged_df' in locals() and not merged_df.empty:\n",
    "        complexity_quality_corr = merged_df['complexity_score'].corr(merged_df['overall_quality_score'])\n",
    "        if complexity_quality_corr > 0.3:\n",
    "            print(f\"   ‚Ä¢ Strong correlation between complexity and quality (r={complexity_quality_corr:.3f})\")\n",
    "            print(f\"     ‚Üí More complex apps tend to be higher quality\")\n",
    "        elif complexity_quality_corr < -0.3:\n",
    "            print(f\"   ‚Ä¢ Negative correlation between complexity and quality (r={complexity_quality_corr:.3f})\")\n",
    "            print(f\"     ‚Üí Simpler apps tend to be higher quality\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ Weak correlation between complexity and quality (r={complexity_quality_corr:.3f})\")\n",
    "            print(f\"     ‚Üí Quality is independent of application complexity\")\n",
    "    \n",
    "    print(f\"\\n‚ú® SUCCESS PATTERNS IDENTIFIED:\")\n",
    "    \n",
    "    # Analyze top performers for patterns\n",
    "    if len(high_quality_apps) > 0:\n",
    "        high_quality_completeness = high_quality_apps['completeness_score'].mean()\n",
    "        high_quality_professional = high_quality_apps['professional_score'].mean()\n",
    "        high_quality_adoption = high_quality_apps['adoption_score'].mean()\n",
    "        \n",
    "        print(f\"   ‚Ä¢ High-quality apps average {high_quality_completeness:.1f}/10 completeness\")\n",
    "        print(f\"   ‚Ä¢ High-quality apps average {high_quality_professional:.1f}/10 professional score\")\n",
    "        print(f\"   ‚Ä¢ High-quality apps average {high_quality_adoption:.1f}/10 adoption score\")\n",
    "        \n",
    "        # Find the key differentiator\n",
    "        completeness_diff = high_quality_completeness - avg_completeness\n",
    "        professional_diff = high_quality_professional - avg_professional\n",
    "        adoption_diff = high_quality_adoption - avg_adoption\n",
    "        \n",
    "        differences = {\n",
    "            'Completeness': completeness_diff,\n",
    "            'Professional': professional_diff,\n",
    "            'Adoption': adoption_diff\n",
    "        }\n",
    "        \n",
    "        biggest_differentiator = max(differences, key=differences.get)\n",
    "        print(f\"\\nüîë KEY SUCCESS FACTOR: {biggest_differentiator}\")\n",
    "        print(f\"   High-quality apps score {differences[biggest_differentiator]:.1f} points higher on average\")\n",
    "    \n",
    "    print(f\"\\nüéì CONCLUSION:\")\n",
    "    if avg_overall_quality >= 7:\n",
    "        print(f\"   Base44 ecosystem shows HIGH QUALITY with avg score {avg_overall_quality:.1f}/10\")\n",
    "    elif avg_overall_quality >= 5:\n",
    "        print(f\"   Base44 ecosystem shows MODERATE QUALITY with avg score {avg_overall_quality:.1f}/10\")\n",
    "    else:\n",
    "        print(f\"   Base44 ecosystem shows DEVELOPING QUALITY with avg score {avg_overall_quality:.1f}/10\")\n",
    "    \n",
    "    print(f\"   Platform is {'mature' if high_quality_percentage > 25 else 'emerging'} with {high_quality_percentage:.1f}% high-quality applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This comprehensive quality analysis of Base44 applications provides valuable insights into the platform's ecosystem:\n",
    "\n",
    "### Key Quality Findings:\n",
    "1. **Overall Quality Distribution**: Understanding the range and average quality scores\n",
    "2. **Quality by Category**: How different application types perform\n",
    "3. **Success Patterns**: Characteristics of high-performing applications\n",
    "4. **Quality Factors**: Most important metrics for success\n",
    "5. **Platform Maturity**: Assessment of the Base44 ecosystem\n",
    "\n",
    "### Quality Metrics Insights:\n",
    "- **Completeness**: How well apps fulfill their stated purpose\n",
    "- **Professional**: Polish, branding, and presentation quality\n",
    "- **Adoption**: User engagement and market reception\n",
    "- **Time-to-Market**: Development speed advantages\n",
    "- **Longevity**: Sustainability and maintenance\n",
    "\n",
    "### Files Generated:\n",
    "- `data/processed/quality_metrics.csv` - Detailed quality scores\n",
    "- `data/processed/quality_report.json` - Comprehensive quality report\n",
    "\n",
    "### Research Implications:\n",
    "This analysis contributes to understanding no-code platform effectiveness and provides empirical evidence for the Base44 phenomenon research question.\n",
    "\n",
    "### Next Steps:\n",
    "1. **Visualization Notebook** - Create comprehensive charts and dashboards\n",
    "2. **Academic Paper** - Synthesize all findings into research publication\n",
    "3. **Presentation** - Prepare findings for academic or business audiences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}